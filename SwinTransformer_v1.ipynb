{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["import math, re, os, random\n","import tensorflow as tf\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from kaggle_datasets import KaggleDatasets\n","from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n","\n","print(\"TF version \" + tf.__version__)\n","AUTO = tf.data.experimental.AUTOTUNE"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:00:19.456378Z","iopub.execute_input":"2022-12-06T08:00:19.457534Z","iopub.status.idle":"2022-12-06T08:00:19.465483Z","shell.execute_reply.started":"2022-12-06T08:00:19.457469Z","shell.execute_reply":"2022-12-06T08:00:19.464045Z"},"trusted":true,"id":"p4yHZpODtaQq","outputId":"58e67917-8bb7-408e-8e79-1a7be29d6c0f"},"execution_count":null,"outputs":[{"name":"stdout","text":"TF version 2.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":["from torchvision.models import resnet50, ResNet50_Weights"],"metadata":{"id":"CjBdF1bI2XyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Detect hardware, return appropriate distribution strategy\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n","    print('Running on TPU ', tpu.master())\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","else:\n","    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","\n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:00:19.474296Z","iopub.execute_input":"2022-12-06T08:00:19.474695Z","iopub.status.idle":"2022-12-06T08:00:26.063183Z","shell.execute_reply.started":"2022-12-06T08:00:19.474655Z","shell.execute_reply":"2022-12-06T08:00:26.062144Z"},"trusted":true,"id":"DGR1F4nrtaQu","outputId":"dba24230-d716-48ce-f3ca-cd55ecbcdf98"},"execution_count":null,"outputs":[{"name":"stdout","text":"Running on TPU  grpc://10.0.0.2:8470\n","output_type":"stream"},{"name":"stderr","text":"2022-12-06 08:00:19.483053: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-12-06 08:00:19.483146: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30284}\n2022-12-06 08:00:19.486085: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-12-06 08:00:19.486148: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30284}\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":["IMAGE_SIZE = [192, 192]\n","EPOCHS = 30\n","BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n","SEED = 752\n","SKIP_VALIDATION = False\n","TTA_NUM = 5\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:00:26.065342Z","iopub.execute_input":"2022-12-06T08:00:26.065631Z","iopub.status.idle":"2022-12-06T08:00:26.071951Z","shell.execute_reply.started":"2022-12-06T08:00:26.065596Z","shell.execute_reply":"2022-12-06T08:00:26.070822Z"},"trusted":true,"id":"k6jmFBJHtaQu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["GCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\n","\n","GCS_PATH_SELECT = { # available image sizes\n","    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n","    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n","    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n","    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n","}\n","GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n","\n","TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\n","VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\n","TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') # predictions on this dataset should be submitted for the competition \n","\n","CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n","           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n","           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n","           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n","           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n","           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n","           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n","           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n","           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n","           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n","           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                          # 100 - 102"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:00:26.073743Z","iopub.execute_input":"2022-12-06T08:00:26.074104Z","iopub.status.idle":"2022-12-06T08:00:26.534297Z","shell.execute_reply.started":"2022-12-06T08:00:26.074067Z","shell.execute_reply":"2022-12-06T08:00:26.533084Z"},"trusted":true,"id":"e5nrlULJtaQv","outputId":"3ce99724-c653-4283-b078-6f39687cac69"},"execution_count":null,"outputs":[{"name":"stderr","text":"2022-12-06 08:00:26.396812: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n2022-12-06 08:00:26.441486: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n2022-12-06 08:00:26.488948: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","output_type":"stream"}]},{"cell_type":"code","source":["# numpy and matplotlib defaults\n","np.set_printoptions(threshold=15, linewidth=80)\n","\n","def batch_to_numpy_images_and_labels(data):\n","    images, labels = data\n","    numpy_images = images.numpy()\n","    numpy_labels = labels.numpy()\n","    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n","        numpy_labels = [None for _ in enumerate(numpy_images)]\n","    # If no labels, only image IDs, return None for labels (this is the case for test data)\n","    return numpy_images, numpy_labels\n","\n","def title_from_label_and_target(label, correct_label):\n","    if correct_label is None:\n","        return CLASSES[label], True\n","    correct = (label == correct_label)\n","    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n","                                CLASSES[correct_label] if not correct else ''), correct\n","\n","def display_one_flower(image, title, subplot, red=False, titlesize=16):\n","    plt.subplot(*subplot)\n","    plt.axis('off')\n","    plt.imshow(image)\n","    if len(title) > 0:\n","        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n","    return (subplot[0], subplot[1], subplot[2]+1)\n","    \n","def display_batch_of_images(databatch, predictions=None):\n","    \"\"\"This will work with:\n","    display_batch_of_images(images)\n","    display_batch_of_images(images, predictions)\n","    display_batch_of_images((images, labels))\n","    display_batch_of_images((images, labels), predictions)\n","    \"\"\"\n","    # data\n","    images, labels = batch_to_numpy_images_and_labels(databatch)\n","    if labels is None:\n","        labels = [None for _ in enumerate(images)]\n","        \n","    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n","    rows = int(math.sqrt(len(images)))\n","    cols = len(images)//rows\n","        \n","    # size and spacing\n","    FIGSIZE = 13.0\n","    SPACING = 0.1\n","    subplot=(rows,cols,1)\n","    if rows < cols:\n","        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n","    else:\n","        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n","    \n","    # display\n","    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n","        title = '' if label is None else CLASSES[label]\n","        correct = True\n","        if predictions is not None:\n","            title, correct = title_from_label_and_target(predictions[i], label)\n","        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n","        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n","    \n","    #layout\n","    plt.tight_layout()\n","    if label is None and predictions is None:\n","        plt.subplots_adjust(wspace=0, hspace=0)\n","    else:\n","        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n","    plt.show()\n","\n","def display_confusion_matrix(cmat, score, precision, recall):\n","    plt.figure(figsize=(15,15))\n","    ax = plt.gca()\n","    ax.matshow(cmat, cmap='Reds')\n","    ax.set_xticks(range(len(CLASSES)))\n","    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n","    ax.set_yticks(range(len(CLASSES)))\n","    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n","    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","    titlestring = \"\"\n","    if score is not None:\n","        titlestring += 'f1 = {:.3f} '.format(score)\n","    if precision is not None:\n","        titlestring += '\\nprecision = {:.3f} '.format(precision)\n","    if recall is not None:\n","        titlestring += '\\nrecall = {:.3f} '.format(recall)\n","    if len(titlestring) > 0:\n","        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n","    plt.show()\n","    \n","def display_training_curves(training, validation, title, subplot):\n","    if subplot%10==1: # set up the subplots on the first call\n","        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n","        plt.tight_layout()\n","    ax = plt.subplot(subplot)\n","    ax.set_facecolor('#F8F8F8')\n","    ax.plot(training)\n","    ax.plot(validation)\n","    ax.set_title('model '+ title)\n","    ax.set_ylabel(title)\n","    #ax.set_ylim(0.28,1.05)\n","    ax.set_xlabel('epoch')\n","    ax.legend(['train', 'valid.'])"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:00:26.536646Z","iopub.execute_input":"2022-12-06T08:00:26.537255Z","iopub.status.idle":"2022-12-06T08:00:26.565551Z","shell.execute_reply.started":"2022-12-06T08:00:26.537191Z","shell.execute_reply":"2022-12-06T08:00:26.564122Z"},"trusted":true,"id":"r6fWD_IMtaQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def decode_image(image_data):\n","    image = tf.image.decode_jpeg(image_data, channels=3)\n","    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n","    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n","    return image\n","\n","def read_labeled_tfrecord(example):\n","    LABELED_TFREC_FORMAT = {\n","        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n","        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n","    }\n","    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n","    image = decode_image(example['image'])\n","    label = tf.cast(example['class'], tf.int32)\n","    return image, label # returns a dataset of (image, label) pairs\n","\n","def read_unlabeled_tfrecord(example):\n","    UNLABELED_TFREC_FORMAT = {\n","        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n","        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n","        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n","    }\n","    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n","    image = decode_image(example['image'])\n","    idnum = example['id']\n","    return image, idnum # returns a dataset of image(s)\n","\n","def load_dataset(filenames, labeled=True, ordered=False):\n","    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n","    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n","\n","    ignore_order = tf.data.Options()\n","    if not ordered:\n","        ignore_order.experimental_deterministic = False # disable order, increase speed\n","\n","    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n","    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n","    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n","    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n","    return dataset\n","\n","def data_augment(image, label):\n","    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n","    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n","    # of the TPU while the TPU itself is computing gradients.\n","\n","    image = tf.image.random_flip_left_right(image, seed=SEED)\n","#     image = random_blockout(image)\n","    return image, label   \n","\n","def get_training_dataset():\n","    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n","#     dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n","#     dataset = dataset.repeat() # the training dataset must repeat for several epochs\n","#     dataset = dataset.shuffle(2048)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n","    return dataset\n","\n","def get_validation_dataset(ordered=False):\n","    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    dataset = dataset.cache()\n","    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n","    return dataset\n","\n","def get_test_dataset(ordered=False):\n","    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n","    return dataset\n","\n","def count_data_items(filenames):\n","    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n","    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n","    return np.sum(n)\n","\n","NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n","NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n","NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n","STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n","print('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:00:26.568742Z","iopub.execute_input":"2022-12-06T08:00:26.569501Z","iopub.status.idle":"2022-12-06T08:00:26.591671Z","shell.execute_reply.started":"2022-12-06T08:00:26.569444Z","shell.execute_reply":"2022-12-06T08:00:26.590501Z"},"trusted":true,"id":"lbAxEu2CtaQx","outputId":"a1e0b467-7c7b-4267-dd5d-8b6ccac310c4"},"execution_count":null,"outputs":[{"name":"stdout","text":"Dataset: 12753 training images, 3712 validation images, 7382 unlabeled test images\n","output_type":"stream"}]},{"cell_type":"code","source":["# data dump\n","print(\"Training data shapes:\")\n","for image, label in get_training_dataset().take(3):\n","    print(image.numpy().shape, label.numpy().shape)\n","print(\"Training data label examples:\", label.numpy())\n","print(\"Validation data shapes:\")\n","for image, label in get_validation_dataset().take(3):\n","    print(image.numpy().shape, label.numpy().shape)\n","print(\"Validation data label examples:\", label.numpy())\n","print(\"Test data shapes:\")\n","for image, idnum in get_test_dataset().take(3):\n","    print(image.numpy().shape, idnum.numpy().shape)\n","print(\"Test data IDs:\", idnum.numpy().astype('U')) # U=unicode string"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:00:26.593228Z","iopub.execute_input":"2022-12-06T08:00:26.593554Z","iopub.status.idle":"2022-12-06T08:00:28.736878Z","shell.execute_reply.started":"2022-12-06T08:00:26.593507Z","shell.execute_reply":"2022-12-06T08:00:28.735453Z"},"trusted":true,"id":"Zm2SUJfktaQx","outputId":"3152da44-26fd-43e0-b900-d8a5bbb29f0b"},"execution_count":null,"outputs":[{"name":"stdout","text":"Training data shapes:\n(128, 192, 192, 3) (128,)\n(128, 192, 192, 3) (128,)\n(128, 192, 192, 3) (128,)\nTraining data label examples: [75 76 81 ... 15 49 67]\nValidation data shapes:\n","output_type":"stream"},{"name":"stderr","text":"2022-12-06 08:00:27.344635: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 636943, Output num: 0\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1670313627.344496440\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 636943, Output num: 0\",\"grpc_status\":3}\n","output_type":"stream"},{"name":"stdout","text":"(128, 192, 192, 3) (128,)\n(128, 192, 192, 3) (128,)\n(128, 192, 192, 3) (128,)\nValidation data label examples: [103  67  53 ...  70  67  52]\nTest data shapes:\n(128, 192, 192, 3) (128,)\n(128, 192, 192, 3) (128,)\n(128, 192, 192, 3) (128,)\nTest data IDs: ['e5f6dbca4' '5c2b3adde' '6557acff6' ... 'b9b1f27a7' '05a947c38' '36c344ab5']\n","output_type":"stream"}]},{"cell_type":"code","source":["# Peek at training data\n","training_dataset = get_training_dataset()\n","training_dataset = training_dataset.unbatch()\n","train_batch = iter(training_dataset)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:00:28.739226Z","iopub.execute_input":"2022-12-06T08:00:28.739672Z","iopub.status.idle":"2022-12-06T08:00:28.789438Z","shell.execute_reply.started":"2022-12-06T08:00:28.739615Z","shell.execute_reply":"2022-12-06T08:00:28.788358Z"},"trusted":true,"id":"zR8hkd5WtaQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_dataset"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:00:28.791425Z","iopub.execute_input":"2022-12-06T08:00:28.791747Z","iopub.status.idle":"2022-12-06T08:00:28.799110Z","shell.execute_reply.started":"2022-12-06T08:00:28.791709Z","shell.execute_reply":"2022-12-06T08:00:28.798105Z"},"trusted":true,"id":"bfHiqlImtaQy","outputId":"145d3cde-90d9-4c0c-f7ac-b61415664c4d"},"execution_count":null,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"<_UnbatchDataset shapes: ((192, 192, 3), ()), types: (tf.float32, tf.int32)>"},"metadata":{}}]},{"cell_type":"code","source":["image = []\n","label = []\n","for last in training_dataset:\n","    images1, labels1 = batch_to_numpy_images_and_labels(last)\n","    image.append(images1)\n","    label.append(labels1)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:00:28.800813Z","iopub.execute_input":"2022-12-06T08:00:28.801120Z","iopub.status.idle":"2022-12-06T08:01:21.710731Z","shell.execute_reply.started":"2022-12-06T08:00:28.801084Z","shell.execute_reply":"2022-12-06T08:01:21.709330Z"},"trusted":true,"id":"4LY0TP-WtaQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(label)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:01:21.712150Z","iopub.execute_input":"2022-12-06T08:01:21.712434Z","iopub.status.idle":"2022-12-06T08:01:21.721987Z","shell.execute_reply.started":"2022-12-06T08:01:21.712403Z","shell.execute_reply":"2022-12-06T08:01:21.720928Z"},"trusted":true,"id":"XS5LscGdtaQz","outputId":"46d1db70-e1d6-4195-bb64-1d2f69d8c0fc"},"execution_count":null,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"12753"},"metadata":{}}]},{"cell_type":"code","source":["test_dataset = get_test_dataset()\n","test_dataset = test_dataset.unbatch()\n","test_batch = iter(test_dataset)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:01:21.723542Z","iopub.execute_input":"2022-12-06T08:01:21.723895Z","iopub.status.idle":"2022-12-06T08:01:21.779770Z","shell.execute_reply.started":"2022-12-06T08:01:21.723858Z","shell.execute_reply":"2022-12-06T08:01:21.778199Z"},"trusted":true,"id":"uICgtegVtaQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from numpy import asarray\n","\n","imaget = []\n","labelt = []\n","for last in test_dataset:\n","    i = last\n","    imaget.append(i.numpy())\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:02:10.597380Z","iopub.execute_input":"2022-12-06T08:02:10.598713Z","iopub.status.idle":"2022-12-06T08:02:11.439987Z","shell.execute_reply.started":"2022-12-06T08:02:10.598645Z","shell.execute_reply":"2022-12-06T08:02:11.438584Z"},"trusted":true,"id":"4qN1_TuntaQz","outputId":"bd5b5f3e-7f9e-4b5f-a13a-2efb43922cd0"},"execution_count":null,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_284/1375375052.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlast\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mimaget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'numpy'"],"ename":"AttributeError","evalue":"'tuple' object has no attribute 'numpy'","output_type":"error"}]},{"cell_type":"code","source":["len(imaget)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:01:27.707675Z","iopub.execute_input":"2022-12-06T08:01:27.708081Z","iopub.status.idle":"2022-12-06T08:01:27.715966Z","shell.execute_reply.started":"2022-12-06T08:01:27.708041Z","shell.execute_reply":"2022-12-06T08:01:27.714737Z"},"trusted":true,"id":"IU0ElkV4taQz","outputId":"1fc86e51-0c30-4d10-e0f1-c2ff9cb9b160"},"execution_count":null,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"7382"},"metadata":{}}]},{"cell_type":"code","source":["!pip install -U tensorflow-addons"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:04:54.333633Z","iopub.execute_input":"2022-12-06T08:04:54.334800Z","iopub.status.idle":"2022-12-06T08:05:15.909140Z","shell.execute_reply.started":"2022-12-06T08:04:54.334721Z","shell.execute_reply":"2022-12-06T08:05:15.907708Z"},"trusted":true,"id":"Hb8vxTx7taQ0","outputId":"6a302341-0942-472c-9627-a57f28fb1b2b"},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.7/site-packages (0.12.1)\nCollecting tensorflow-addons\n  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 4.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (21.0)\nRequirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (2.12.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow-addons) (2.4.7)\nInstalling collected packages: tensorflow-addons\n  Attempting uninstall: tensorflow-addons\n    Found existing installation: tensorflow-addons 0.12.1\n    Uninstalling tensorflow-addons-0.12.1:\n      Successfully uninstalled tensorflow-addons-0.12.1\nSuccessfully installed tensorflow-addons-0.18.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","# import tensorflow_addons as tfa\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:05:25.715206Z","iopub.execute_input":"2022-12-06T08:05:25.715572Z","iopub.status.idle":"2022-12-06T08:05:25.722034Z","shell.execute_reply.started":"2022-12-06T08:05:25.715537Z","shell.execute_reply":"2022-12-06T08:05:25.720975Z"},"trusted":true,"id":"ZrcFdOajtaQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = 104\n","input_shape = (192, 192, 3)\n","patch_size = (2, 2)  # 2-by-2 sized patches\n","dropout_rate = 0.03  # Dropout rate\n","num_heads = 8  # Attention heads\n","embed_dim = 64  # Embedding dimension\n","num_mlp = 256  # MLP layer size\n","qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n","window_size = 2  # Size of attention window\n","shift_size = 1  # Size of shifting window\n","image_dimension = 32  # Initial image size\n","\n","num_patch_x = input_shape[0] // patch_size[0]\n","num_patch_y = input_shape[1] // patch_size[1]\n","\n","learning_rate = 1e-3\n","batch_size = 128\n","num_epochs = 40\n","validation_split = 0.1\n","weight_decay = 0.0001\n","label_smoothing = 0.1"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:06:32.978106Z","iopub.execute_input":"2022-12-06T08:06:32.978952Z","iopub.status.idle":"2022-12-06T08:06:32.987709Z","shell.execute_reply.started":"2022-12-06T08:06:32.978885Z","shell.execute_reply":"2022-12-06T08:06:32.986833Z"},"trusted":true,"id":"qpA_Ez3PtaQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def window_partition(x, window_size):\n","    _, height, width, channels = x.shape\n","    patch_num_y = height // window_size\n","    patch_num_x = width // window_size\n","    x = tf.reshape(\n","        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n","    )\n","    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n","    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, height, width, channels):\n","    patch_num_y = height // window_size\n","    patch_num_x = width // window_size\n","    x = tf.reshape(\n","        windows,\n","        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n","    )\n","    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n","    x = tf.reshape(x, shape=(-1, height, width, channels))\n","    return x\n","\n","\n","class DropPath(layers.Layer):\n","    def __init__(self, drop_prob=None, **kwargs):\n","        super(DropPath, self).__init__(**kwargs)\n","        self.drop_prob = drop_prob\n","\n","    def call(self, x):\n","        input_shape = tf.shape(x)\n","        batch_size = input_shape[0]\n","        rank = x.shape.rank\n","        shape = (batch_size,) + (1,) * (rank - 1)\n","        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n","        path_mask = tf.floor(random_tensor)\n","        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n","        return output\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:06:49.172039Z","iopub.execute_input":"2022-12-06T08:06:49.172631Z","iopub.status.idle":"2022-12-06T08:06:49.185449Z","shell.execute_reply.started":"2022-12-06T08:06:49.172592Z","shell.execute_reply":"2022-12-06T08:06:49.184377Z"},"trusted":true,"id":"y2BnoqsqtaQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class WindowAttention(layers.Layer):\n","    def __init__(\n","        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n","    ):\n","        super(WindowAttention, self).__init__(**kwargs)\n","        self.dim = dim\n","        self.window_size = window_size\n","        self.num_heads = num_heads\n","        self.scale = (dim // num_heads) ** -0.5\n","        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n","        self.dropout = layers.Dropout(dropout_rate)\n","        self.proj = layers.Dense(dim)\n","\n","    def build(self, input_shape):\n","        num_window_elements = (2 * self.window_size[0] - 1) * (\n","            2 * self.window_size[1] - 1\n","        )\n","        self.relative_position_bias_table = self.add_weight(\n","            shape=(num_window_elements, self.num_heads),\n","            initializer=tf.initializers.Zeros(),\n","            trainable=True,\n","        )\n","        coords_h = np.arange(self.window_size[0])\n","        coords_w = np.arange(self.window_size[1])\n","        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n","        coords = np.stack(coords_matrix)\n","        coords_flatten = coords.reshape(2, -1)\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","        relative_coords = relative_coords.transpose([1, 2, 0])\n","        relative_coords[:, :, 0] += self.window_size[0] - 1\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)\n","\n","        self.relative_position_index = tf.Variable(\n","            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n","        )\n","\n","    def call(self, x, mask=None):\n","        _, size, channels = x.shape\n","        head_dim = channels // self.num_heads\n","        x_qkv = self.qkv(x)\n","        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n","        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n","        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n","        q = q * self.scale\n","        k = tf.transpose(k, perm=(0, 1, 3, 2))\n","        attn = q @ k\n","\n","        num_window_elements = self.window_size[0] * self.window_size[1]\n","        relative_position_index_flat = tf.reshape(\n","            self.relative_position_index, shape=(-1,)\n","        )\n","        relative_position_bias = tf.gather(\n","            self.relative_position_bias_table, relative_position_index_flat\n","        )\n","        relative_position_bias = tf.reshape(\n","            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n","        )\n","        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n","        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n","\n","        if mask is not None:\n","            nW = mask.get_shape()[0]\n","            mask_float = tf.cast(\n","                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n","            )\n","            attn = (\n","                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n","                + mask_float\n","            )\n","            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n","            attn = keras.activations.softmax(attn, axis=-1)\n","        else:\n","            attn = keras.activations.softmax(attn, axis=-1)\n","        attn = self.dropout(attn)\n","\n","        x_qkv = attn @ v\n","        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n","        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n","        x_qkv = self.proj(x_qkv)\n","        x_qkv = self.dropout(x_qkv)\n","        return x_qkv\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:07:13.095019Z","iopub.execute_input":"2022-12-06T08:07:13.095610Z","iopub.status.idle":"2022-12-06T08:07:13.117315Z","shell.execute_reply.started":"2022-12-06T08:07:13.095570Z","shell.execute_reply":"2022-12-06T08:07:13.116141Z"},"trusted":true,"id":"xdJyBpkztaQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SwinTransformer(layers.Layer):\n","    def __init__(\n","        self,\n","        dim,\n","        num_patch,\n","        num_heads,\n","        window_size=7,\n","        shift_size=0,\n","        num_mlp=1024,\n","        qkv_bias=True,\n","        dropout_rate=0.0,\n","        **kwargs,\n","    ):\n","        super(SwinTransformer, self).__init__(**kwargs)\n","\n","        self.dim = dim  # number of input dimensions\n","        self.num_patch = num_patch  # number of embedded patches\n","        self.num_heads = num_heads  # number of attention heads\n","        self.window_size = window_size  # size of window\n","        self.shift_size = shift_size  # size of window shift\n","        self.num_mlp = num_mlp  # number of MLP nodes\n","\n","        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=(self.window_size, self.window_size),\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            dropout_rate=dropout_rate,\n","        )\n","        self.drop_path = DropPath(dropout_rate)\n","        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n","\n","        self.mlp = keras.Sequential(\n","            [\n","                layers.Dense(num_mlp),\n","                layers.Activation(keras.activations.gelu),\n","                layers.Dropout(dropout_rate),\n","                layers.Dense(dim),\n","                layers.Dropout(dropout_rate),\n","            ]\n","        )\n","\n","        if min(self.num_patch) < self.window_size:\n","            self.shift_size = 0\n","            self.window_size = min(self.num_patch)\n","\n","    def build(self, input_shape):\n","        if self.shift_size == 0:\n","            self.attn_mask = None\n","        else:\n","            height, width = self.num_patch\n","            h_slices = (\n","                slice(0, -self.window_size),\n","                slice(-self.window_size, -self.shift_size),\n","                slice(-self.shift_size, None),\n","            )\n","            w_slices = (\n","                slice(0, -self.window_size),\n","                slice(-self.window_size, -self.shift_size),\n","                slice(-self.shift_size, None),\n","            )\n","            mask_array = np.zeros((1, height, width, 1))\n","            count = 0\n","            for h in h_slices:\n","                for w in w_slices:\n","                    mask_array[:, h, w, :] = count\n","                    count += 1\n","            mask_array = tf.convert_to_tensor(mask_array)\n","\n","            # mask array to windows\n","            mask_windows = window_partition(mask_array, self.window_size)\n","            mask_windows = tf.reshape(\n","                mask_windows, shape=[-1, self.window_size * self.window_size]\n","            )\n","            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n","                mask_windows, axis=2\n","            )\n","            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n","            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n","            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n","\n","    def call(self, x):\n","        height, width = self.num_patch\n","        _, num_patches_before, channels = x.shape\n","        x_skip = x\n","        x = self.norm1(x)\n","        x = tf.reshape(x, shape=(-1, height, width, channels))\n","        if self.shift_size > 0:\n","            shifted_x = tf.roll(\n","                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n","            )\n","        else:\n","            shifted_x = x\n","\n","        x_windows = window_partition(shifted_x, self.window_size)\n","        x_windows = tf.reshape(\n","            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n","        )\n","        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n","\n","        attn_windows = tf.reshape(\n","            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n","        )\n","        shifted_x = window_reverse(\n","            attn_windows, self.window_size, height, width, channels\n","        )\n","        if self.shift_size > 0:\n","            x = tf.roll(\n","                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n","            )\n","        else:\n","            x = shifted_x\n","\n","        x = tf.reshape(x, shape=(-1, height * width, channels))\n","        x = self.drop_path(x)\n","        x = x_skip + x\n","        x_skip = x\n","        x = self.norm2(x)\n","        x = self.mlp(x)\n","        x = self.drop_path(x)\n","        x = x_skip + x\n","        return x\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:07:31.621677Z","iopub.execute_input":"2022-12-06T08:07:31.622095Z","iopub.status.idle":"2022-12-06T08:07:31.646519Z","shell.execute_reply.started":"2022-12-06T08:07:31.622055Z","shell.execute_reply":"2022-12-06T08:07:31.645023Z"},"trusted":true,"id":"UqyV160_taQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PatchExtract(layers.Layer):\n","    def __init__(self, patch_size, **kwargs):\n","        super(PatchExtract, self).__init__(**kwargs)\n","        self.patch_size_x = patch_size[0]\n","        self.patch_size_y = patch_size[0]\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n","            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n","            rates=(1, 1, 1, 1),\n","            padding=\"VALID\",\n","        )\n","        patch_dim = patches.shape[-1]\n","        patch_num = patches.shape[1]\n","        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n","\n","\n","class PatchEmbedding(layers.Layer):\n","    def __init__(self, num_patch, embed_dim, **kwargs):\n","        super(PatchEmbedding, self).__init__(**kwargs)\n","        self.num_patch = num_patch\n","        self.proj = layers.Dense(embed_dim)\n","        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n","\n","    def call(self, patch):\n","        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n","        return self.proj(patch) + self.pos_embed(pos)\n","\n","\n","class PatchMerging(tf.keras.layers.Layer):\n","    def __init__(self, num_patch, embed_dim):\n","        super(PatchMerging, self).__init__()\n","        self.num_patch = num_patch\n","        self.embed_dim = embed_dim\n","        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n","\n","    def call(self, x):\n","        height, width = self.num_patch\n","        _, _, C = x.get_shape().as_list()\n","        x = tf.reshape(x, shape=(-1, height, width, C))\n","        x0 = x[:, 0::2, 0::2, :]\n","        x1 = x[:, 1::2, 0::2, :]\n","        x2 = x[:, 0::2, 1::2, :]\n","        x3 = x[:, 1::2, 1::2, :]\n","        x = tf.concat((x0, x1, x2, x3), axis=-1)\n","        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n","        return self.linear_trans(x)\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:07:42.881229Z","iopub.execute_input":"2022-12-06T08:07:42.882322Z","iopub.status.idle":"2022-12-06T08:07:42.899676Z","shell.execute_reply.started":"2022-12-06T08:07:42.882256Z","shell.execute_reply":"2022-12-06T08:07:42.898612Z"},"trusted":true,"id":"ZwKbBRLhtaQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input = layers.Input(input_shape)\n","# x = layers.RandomCrop(image_dimension, image_dimension)(input)\n","# x = layers.RandomFlip(\"horizontal\")(x)\n","x = PatchExtract(patch_size)(input)\n","x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n","x = SwinTransformer(\n","    dim=embed_dim,\n","    num_patch=(num_patch_x, num_patch_y),\n","    num_heads=num_heads,\n","    window_size=window_size,\n","    shift_size=0,\n","    num_mlp=num_mlp,\n","    qkv_bias=qkv_bias,\n","    dropout_rate=dropout_rate,\n",")(x)\n","x = SwinTransformer(\n","    dim=embed_dim,\n","    num_patch=(num_patch_x, num_patch_y),\n","    num_heads=num_heads,\n","    window_size=window_size,\n","    shift_size=shift_size,\n","    num_mlp=num_mlp,\n","    qkv_bias=qkv_bias,\n","    dropout_rate=dropout_rate,\n",")(x)\n","x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","output = layers.Dense(num_classes, activation=\"softmax\")(x)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:08:04.759843Z","iopub.execute_input":"2022-12-06T08:08:04.760289Z","iopub.status.idle":"2022-12-06T08:08:06.500063Z","shell.execute_reply.started":"2022-12-06T08:08:04.760241Z","shell.execute_reply":"2022-12-06T08:08:06.499002Z"},"trusted":true,"id":"JmR0_22ataQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy\n","x_train = numpy.array(image)\n","y_train = numpy.array(label)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:18:10.520129Z","iopub.execute_input":"2022-12-06T08:18:10.521137Z","iopub.status.idle":"2022-12-06T08:18:10.645966Z","shell.execute_reply.started":"2022-12-06T08:18:10.521087Z","shell.execute_reply":"2022-12-06T08:18:10.644889Z"},"trusted":true,"id":"3W51d5lstaQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train = keras.utils.to_categorical(y_train, 104)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:20:37.382132Z","iopub.execute_input":"2022-12-06T08:20:37.383069Z","iopub.status.idle":"2022-12-06T08:20:37.388376Z","shell.execute_reply.started":"2022-12-06T08:20:37.383023Z","shell.execute_reply":"2022-12-06T08:20:37.387355Z"},"trusted":true,"id":"fEOBwPAUtaQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = keras.Model(input, output)\n","model.compile(\n","    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n","    optimizer=tf.keras.optimizers.Adam(\n","        learning_rate=learning_rate\n","    ),\n","    metrics=[\n","        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n","    ],\n",")\n","\n","history = model.fit(\n","    x_train,\n","    y_train,\n","    batch_size=batch_size,\n","    epochs=100)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:25:08.780719Z","iopub.execute_input":"2022-12-06T08:25:08.781132Z","iopub.status.idle":"2022-12-06T08:34:06.484359Z","shell.execute_reply.started":"2022-12-06T08:25:08.781090Z","shell.execute_reply":"2022-12-06T08:34:06.483017Z"},"trusted":true,"id":"Oht09a8QtaQ2","outputId":"f60358b6-ebb8-400e-bf36-867feead4d2a"},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n1/1 [==============================] - 16s 16s/step - loss: 3.0320 - accuracy: 0.2891\nEpoch 2/100\n1/1 [==============================] - 5s 5s/step - loss: 3.3857 - accuracy: 0.2109\nEpoch 3/100\n1/1 [==============================] - 5s 5s/step - loss: 3.4099 - accuracy: 0.1719\nEpoch 4/100\n1/1 [==============================] - 5s 5s/step - loss: 3.1339 - accuracy: 0.2344\nEpoch 5/100\n1/1 [==============================] - 5s 5s/step - loss: 3.1050 - accuracy: 0.2344\nEpoch 6/100\n1/1 [==============================] - 5s 5s/step - loss: 3.0649 - accuracy: 0.2500\nEpoch 7/100\n1/1 [==============================] - 5s 5s/step - loss: 3.0964 - accuracy: 0.2266\nEpoch 8/100\n1/1 [==============================] - 5s 5s/step - loss: 2.9910 - accuracy: 0.2812\nEpoch 9/100\n1/1 [==============================] - 5s 5s/step - loss: 2.9620 - accuracy: 0.2734\nEpoch 10/100\n1/1 [==============================] - 5s 5s/step - loss: 2.9462 - accuracy: 0.2578\nEpoch 11/100\n1/1 [==============================] - 5s 5s/step - loss: 2.9298 - accuracy: 0.2422\nEpoch 12/100\n1/1 [==============================] - 5s 5s/step - loss: 2.9376 - accuracy: 0.2656\nEpoch 13/100\n1/1 [==============================] - 5s 5s/step - loss: 2.8569 - accuracy: 0.3047\nEpoch 14/100\n1/1 [==============================] - 5s 5s/step - loss: 2.8252 - accuracy: 0.3281\nEpoch 15/100\n1/1 [==============================] - 5s 5s/step - loss: 2.8014 - accuracy: 0.3047\nEpoch 16/100\n1/1 [==============================] - 5s 5s/step - loss: 2.6966 - accuracy: 0.3906\nEpoch 17/100\n1/1 [==============================] - 5s 5s/step - loss: 2.7314 - accuracy: 0.3359\nEpoch 18/100\n1/1 [==============================] - 5s 5s/step - loss: 2.7201 - accuracy: 0.2891\nEpoch 19/100\n1/1 [==============================] - 5s 5s/step - loss: 2.6887 - accuracy: 0.3672\nEpoch 20/100\n1/1 [==============================] - 5s 5s/step - loss: 2.6832 - accuracy: 0.3906\nEpoch 21/100\n1/1 [==============================] - 5s 5s/step - loss: 2.6377 - accuracy: 0.4219\nEpoch 22/100\n1/1 [==============================] - 5s 5s/step - loss: 2.5647 - accuracy: 0.4062\nEpoch 23/100\n1/1 [==============================] - 5s 5s/step - loss: 2.5231 - accuracy: 0.4453\nEpoch 24/100\n1/1 [==============================] - 5s 5s/step - loss: 2.4618 - accuracy: 0.4609\nEpoch 25/100\n1/1 [==============================] - 5s 5s/step - loss: 2.4915 - accuracy: 0.4297\nEpoch 26/100\n1/1 [==============================] - 5s 5s/step - loss: 2.3709 - accuracy: 0.4766\nEpoch 27/100\n1/1 [==============================] - 5s 5s/step - loss: 2.3869 - accuracy: 0.4297\nEpoch 28/100\n1/1 [==============================] - 5s 5s/step - loss: 2.3763 - accuracy: 0.4531\nEpoch 29/100\n1/1 [==============================] - 5s 5s/step - loss: 2.3399 - accuracy: 0.5000\nEpoch 30/100\n1/1 [==============================] - 5s 5s/step - loss: 2.2616 - accuracy: 0.5078\nEpoch 31/100\n1/1 [==============================] - 5s 5s/step - loss: 2.3439 - accuracy: 0.4688\nEpoch 32/100\n1/1 [==============================] - 5s 5s/step - loss: 2.1927 - accuracy: 0.5234\nEpoch 33/100\n1/1 [==============================] - 5s 5s/step - loss: 2.2432 - accuracy: 0.5547\nEpoch 34/100\n1/1 [==============================] - 5s 5s/step - loss: 2.2179 - accuracy: 0.5859\nEpoch 35/100\n1/1 [==============================] - 5s 5s/step - loss: 2.0761 - accuracy: 0.6562\nEpoch 36/100\n1/1 [==============================] - 5s 5s/step - loss: 2.1303 - accuracy: 0.6172\nEpoch 37/100\n1/1 [==============================] - 5s 5s/step - loss: 2.0859 - accuracy: 0.6250\nEpoch 38/100\n1/1 [==============================] - 5s 5s/step - loss: 2.0791 - accuracy: 0.6328\nEpoch 39/100\n1/1 [==============================] - 5s 5s/step - loss: 1.9921 - accuracy: 0.6719\nEpoch 40/100\n1/1 [==============================] - 5s 5s/step - loss: 1.9900 - accuracy: 0.7109\nEpoch 41/100\n1/1 [==============================] - 5s 5s/step - loss: 1.9852 - accuracy: 0.6797\nEpoch 42/100\n1/1 [==============================] - 5s 5s/step - loss: 1.9285 - accuracy: 0.7109\nEpoch 43/100\n1/1 [==============================] - 5s 5s/step - loss: 1.8609 - accuracy: 0.7578\nEpoch 44/100\n1/1 [==============================] - 5s 5s/step - loss: 1.9217 - accuracy: 0.7266\nEpoch 45/100\n1/1 [==============================] - 5s 5s/step - loss: 1.7994 - accuracy: 0.7891\nEpoch 46/100\n1/1 [==============================] - 5s 5s/step - loss: 1.8124 - accuracy: 0.7891\nEpoch 47/100\n1/1 [==============================] - 5s 5s/step - loss: 1.7323 - accuracy: 0.7734\nEpoch 48/100\n1/1 [==============================] - 5s 5s/step - loss: 1.7022 - accuracy: 0.7891\nEpoch 49/100\n1/1 [==============================] - 5s 5s/step - loss: 1.6729 - accuracy: 0.8281\nEpoch 50/100\n1/1 [==============================] - 5s 5s/step - loss: 1.7617 - accuracy: 0.7891\nEpoch 51/100\n1/1 [==============================] - 5s 5s/step - loss: 1.6554 - accuracy: 0.8438\nEpoch 52/100\n1/1 [==============================] - 5s 5s/step - loss: 1.6858 - accuracy: 0.8438\nEpoch 53/100\n1/1 [==============================] - 5s 5s/step - loss: 1.6057 - accuracy: 0.8750\nEpoch 54/100\n1/1 [==============================] - 5s 5s/step - loss: 1.6883 - accuracy: 0.8203\nEpoch 55/100\n1/1 [==============================] - 5s 5s/step - loss: 1.5960 - accuracy: 0.8750\nEpoch 56/100\n1/1 [==============================] - 5s 5s/step - loss: 1.6117 - accuracy: 0.8594\nEpoch 57/100\n1/1 [==============================] - 5s 5s/step - loss: 1.5028 - accuracy: 0.8984\nEpoch 58/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4145 - accuracy: 0.9375\nEpoch 59/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4497 - accuracy: 0.9141\nEpoch 60/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4654 - accuracy: 0.9062\nEpoch 61/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4874 - accuracy: 0.8906\nEpoch 62/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4925 - accuracy: 0.9062\nEpoch 63/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3822 - accuracy: 0.9453\nEpoch 64/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4070 - accuracy: 0.9219\nEpoch 65/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4035 - accuracy: 0.9297\nEpoch 66/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3743 - accuracy: 0.9609\nEpoch 67/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4068 - accuracy: 0.9375\nEpoch 68/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4194 - accuracy: 0.9141\nEpoch 69/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4647 - accuracy: 0.9375\nEpoch 70/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3788 - accuracy: 0.9375\nEpoch 71/100\n1/1 [==============================] - 5s 5s/step - loss: 1.4445 - accuracy: 0.9219\nEpoch 72/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3238 - accuracy: 0.9375\nEpoch 73/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2580 - accuracy: 0.9844\nEpoch 74/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3132 - accuracy: 0.9375\nEpoch 75/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3286 - accuracy: 0.9453\nEpoch 76/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3655 - accuracy: 0.9297\nEpoch 77/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3454 - accuracy: 0.9531\nEpoch 78/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3190 - accuracy: 0.9531\nEpoch 79/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2952 - accuracy: 0.9453\nEpoch 80/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2343 - accuracy: 0.9688\nEpoch 81/100\n1/1 [==============================] - 5s 5s/step - loss: 1.1679 - accuracy: 0.9922\nEpoch 82/100\n1/1 [==============================] - 5s 5s/step - loss: 1.1755 - accuracy: 0.9766\nEpoch 83/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2353 - accuracy: 0.9453\nEpoch 84/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3902 - accuracy: 0.9453\nEpoch 85/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2678 - accuracy: 0.9688\nEpoch 86/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2242 - accuracy: 0.9609\nEpoch 87/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2448 - accuracy: 0.9453\nEpoch 88/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2009 - accuracy: 0.9766\nEpoch 89/100\n1/1 [==============================] - 5s 5s/step - loss: 1.1697 - accuracy: 0.9688\nEpoch 90/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3078 - accuracy: 0.9375\nEpoch 91/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2489 - accuracy: 0.9531\nEpoch 92/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2276 - accuracy: 0.9609\nEpoch 93/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2317 - accuracy: 0.9531\nEpoch 94/100\n1/1 [==============================] - 5s 5s/step - loss: 1.1571 - accuracy: 0.9531\nEpoch 95/100\n1/1 [==============================] - 5s 5s/step - loss: 1.2246 - accuracy: 0.9609\nEpoch 96/100\n1/1 [==============================] - 5s 5s/step - loss: 1.1761 - accuracy: 0.9609\nEpoch 97/100\n1/1 [==============================] - 5s 5s/step - loss: 1.1474 - accuracy: 0.9766\nEpoch 98/100\n1/1 [==============================] - 5s 5s/step - loss: 1.1643 - accuracy: 0.9766\nEpoch 99/100\n1/1 [==============================] - 5s 5s/step - loss: 1.3235 - accuracy: 0.9297\nEpoch 100/100\n1/1 [==============================] - 5s 5s/step - loss: 1.1676 - accuracy: 0.9609\n","output_type":"stream"}]},{"cell_type":"code","source":["def predict_tta(model, n_iter):\n","    probs  = []\n","    for i in range(n_iter):\n","        test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n","        test_images_ds = test_ds.map(lambda image, idnum: image)\n","        probs.append(model.predict(test_images_ds,verbose=0))\n","        \n","    return probs\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:34:21.897540Z","iopub.execute_input":"2022-12-06T08:34:21.898706Z","iopub.status.idle":"2022-12-06T08:34:21.906820Z","shell.execute_reply.started":"2022-12-06T08:34:21.898648Z","shell.execute_reply":"2022-12-06T08:34:21.905866Z"},"trusted":true,"id":"6DbAXmrktaQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_ds = get_test_dataset(ordered=True)\n","\n","print('Computing predictions...')\n","test_images_ds = test_ds.map(lambda image, idnum: image)\n","probabilities = model.predict(test_images_ds)\n","predictions = np.argmax(probabilities, axis=-1)\n","print(predictions)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:40:21.013241Z","iopub.execute_input":"2022-12-06T08:40:21.014712Z","iopub.status.idle":"2022-12-06T08:41:35.518390Z","shell.execute_reply.started":"2022-12-06T08:40:21.014645Z","shell.execute_reply":"2022-12-06T08:41:35.517022Z"},"trusted":true,"id":"LVU-tYhUtaQ2","outputId":"06fc0504-d98e-4c94-a8ec-a779f0929240"},"execution_count":null,"outputs":[{"name":"stdout","text":"Computing predictions...\n[  4 102  74 ...  63  47  49]\n","output_type":"stream"},{"name":"stderr","text":"2022-12-06 08:41:35.512762: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 687316, Output num: 0\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1670316095.512640029\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 687316, Output num: 0\",\"grpc_status\":3}\n","output_type":"stream"}]},{"cell_type":"code","source":["print('Generating submission.csv file...')\n","\n","# Get image ids from test set and convert to unicode\n","test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n","test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n","\n","# Write the submission file\n","np.savetxt(\n","    'submission.csv',\n","    np.rec.fromarrays([test_ids, predictions]),\n","    fmt=['%s', '%d'],\n","    delimiter=',',\n","    header='id,label',\n","    comments='',\n",")\n","\n","# Look at the first few predictions\n","!head submission.csv"],"metadata":{"execution":{"iopub.status.busy":"2022-12-06T08:41:37.285461Z","iopub.execute_input":"2022-12-06T08:41:37.285842Z","iopub.status.idle":"2022-12-06T08:41:39.002215Z","shell.execute_reply.started":"2022-12-06T08:41:37.285796Z","shell.execute_reply":"2022-12-06T08:41:39.000736Z"},"trusted":true,"id":"GMMQtwUctaQ2","outputId":"789525e1-211d-45a3-987f-49da5cd7566a"},"execution_count":null,"outputs":[{"name":"stdout","text":"Generating submission.csv file...\nid,label\n252d840db,4\n1c4736dea,102\nc37a6f3e9,74\n00e4f514e,103\n59d1b6146,53\n8d808a07b,4\naeb67eefb,47\n53cfc6586,63\naaa580243,33\n","output_type":"stream"}]},{"cell_type":"code","source":[],"metadata":{"id":"Gwy4kFNmtaQ2"},"execution_count":null,"outputs":[]}]}